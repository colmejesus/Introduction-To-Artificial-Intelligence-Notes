An agent should strive to do the 'right' thing, based on what it can perceive and the actions it can perform. 

It is very hard to describe what 'right' is as being right is a very subjective matter, something right in one scenario could be wrong in another, therefore it gets really hard to define what 'right' might be, especially more so in the context of defining right, computationally since our aim is to embed this behavior in the intelligent systems we build. How can we hope to solve this problem? We can start by defining a performance measure.

A performance measure is a criterion for success of an agent's behaviors. For example, performance of a vacuum cleaner agent could be the amount of dirt cleaned up, amount of time taken, amount of electricity consumed, amount of noise generated etc. 

Therefore, we can create an objective system for A.I. systems which will have these various components to gather information and perform certain actions it is told to, and then we can measure its actions against the performance measures we set  to see how successful it was.

Here's how we can define how an Ideal Rational Agent expected to behave:

![[ideal rational agent.png]]

Looking at this definition, we can say that the Goal of an Ideal Rational agent is to maximize and optimize its objective function, and it can only optimize it's objective functions by taking actions i.e. learn from what it does and perform better next time. 